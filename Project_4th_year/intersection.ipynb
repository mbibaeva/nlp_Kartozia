{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('reviews.txt', 'r', encoding='utf-8')\n",
    "corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file2 = open('seed.txt', 'r', encoding='utf-8')\n",
    "seed = file2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('rusentilex.csv','r',encoding='utf-8') as file3:\n",
    "    sentiment_dict = {word.split(',')[0] for word in file3.readlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = set(seed.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'И пускай на меня не обижается наш прославленный защитник - франкофон «Монреаль Канадиенс» Maxime – я'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "class TextNormalizer(TransformerMixin):\n",
    "    def __init__(self, stop_words=stopwords.words('english'), \n",
    "                 stemmer=SnowballStemmer(\"english\")):\n",
    "        self.stop_words_ = stop_words\n",
    "        self.stemmer_ = stemmer\n",
    "        self.regex_ = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    # by convention, must return self\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def _normalize(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.regex_.sub(' ', text)\n",
    "        text = [self.stemmer_.stem(word) for word in text.split() if word not in self.stop_words_]\n",
    "        return \" \".join(text)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array(list(map(lambda x: self._normalize(x), X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some wrappers to work with word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "#from glove import Corpus, Glove\n",
    "\n",
    "\n",
    "# convert words from tweet to vectors and average them over tweet\n",
    "# or use weighted average according to idf\n",
    "class Text2Vec(TransformerMixin):\n",
    "    def __init__(self, vectorizer, use_idf=False):\n",
    "        self.use_idf_ = use_idf\n",
    "        self.w2v = vectorizer\n",
    "        self.size_ = self.w2v.get_size()\n",
    "        self.weights_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.w2v.fit(X)\n",
    "        \n",
    "        if self.use_idf_:\n",
    "            tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "            tfidf.fit(X)\n",
    "            # if a word was never seen - it must be at least as infrequent\n",
    "            # as any of the known words - so the default idf is the max of known idf's\n",
    "            max_idf = max(tfidf.idf_)\n",
    "            self.weights_ = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.w2v is None:\n",
    "            raise Exception('model not fitted')\n",
    "            \n",
    "        if self.use_idf_:\n",
    "            if self.weights_ is None:\n",
    "                raise Exception('model not fitted')\n",
    "                \n",
    "            result = np.array([np.mean([self.w2v.transform(w) * self.weights_[w] for w in sentence.split() if self.w2v.has(w)]\n",
    "                                       or [np.zeros(self.size_)], axis=0) for sentence in X])\n",
    "        else:\n",
    "            result = np.array([np.mean([self.w2v.transform(w) for w in sentence.split() if self.w2v.has(w)] \n",
    "                                       or [np.zeros(self.size_)], axis=0) for sentence in X])\n",
    "        return result\n",
    "    \n",
    "    \n",
    "class Word2VecWrapper(TransformerMixin):\n",
    "    def __init__(self, window=5,negative=5, size=100, iter=100, is_cbow=False, random_state=SEED):\n",
    "        self.window_ = window\n",
    "        self.negative_ = negative\n",
    "        self.size_ = size\n",
    "        self.iter_ = iter\n",
    "        self.is_cbow_ = is_cbow\n",
    "        self.w2v = None\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def get_size(self):\n",
    "        return self.size_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        X: list of strings\n",
    "        \"\"\"\n",
    "        sentences_list = [x.split() for x in X]\n",
    "        self.w2v = Word2Vec(sentences_list, \n",
    "                            window=self.window_,\n",
    "                            negative=self.negative_, \n",
    "                            size=self.size_, \n",
    "                            iter=self.iter_,\n",
    "                            sg=not self.is_cbow_, seed=self.random_state)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def has(self, word):\n",
    "        return word in self.w2v\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        X: a word\n",
    "        \"\"\"\n",
    "        if self.w2v is None:\n",
    "            raise Exception('model not fitted')\n",
    "        return self.w2v[X] if X in self.w2v else np.zeros(self.size_)\n",
    "    \n",
    "\n",
    "class WordEmbeddingFabric:\n",
    "    @classmethod\n",
    "    def create(cls, method, size, cbow, n_iter=300):\n",
    "        if method == 'word2vec':\n",
    "            return Word2VecWrapper(window=3, negative=5, size=size, iter=n_iter, is_cbow=cbow)\n",
    "                    \n",
    "        if method == 'glove':\n",
    "            return GloveWrapper(window=3, learning_rate=0.05, size=size, epochs=n_iter, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'самый', 'уже', 'по', 'этот', 'по-прежнему', 'и', 'во', 'пока', 'главный', 'со', 'каждый', 'ли', 'в', 'после', 'даже', 'едва', 'примечательно', 'над', 'свой', 'ещё', 'она', 'на', 'два', 'восемь', 'что', 'здесь', 'стать', 'лишь', 'еще', 'рамка', 'он', 'год', 'я', 'с', 'первый', 'а', 'четыре'}\n"
     ]
    }
   ],
   "source": [
    "def load_stop_words(stop_filename):\n",
    "\t''' загрузить список стоп-слов из файла, одно слово на строке '''\n",
    "\twith open(stop_filename, encoding = 'utf-8') as f:\n",
    "\t\tstopwords = [w.strip() for w in f.readlines()]\n",
    "\treturn set(stopwords)\n",
    "\n",
    "stop_words = load_stop_words(\"stoplist_russian.txt\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "def preprocessing(raw_text):\n",
    "    clean_text = re.sub('\\W+', ' ', raw_text) # \\W = [^a-zA-Z0-9_]\n",
    "    return clean_text\n",
    "\n",
    "def lemmatize(input):\n",
    "    return [lemma.strip() for lemma in m.lemmatize(input.lower()) if lemma.strip()]\n",
    "\n",
    "def remove_stop_words(lemmas, stopwords):\n",
    "    return ' '.join([word for word in lemmas if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove_stop_words(lemmatize(corpus), stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus_lemm = pd.read_csv('reviews_score.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>food</th>\n",
       "      <th>interior</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17600</td>\n",
       "      <td>и пускай на я не обижаться наш прославленный з...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23518</td>\n",
       "      <td>- здравствовать. виа д’арженто! - добрый вечер...</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>27221</td>\n",
       "      <td>советовать вы увольнять ваш метродотель елена,...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29097</td>\n",
       "      <td>отличный средне вековый интеръер. приятный обс...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>23065</td>\n",
       "      <td>ужинать в ресторан баден-баден 6 март . импоза...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                             review  food  \\\n",
       "0           0  17600  и пускай на я не обижаться наш прославленный з...     8   \n",
       "1           1  23518  - здравствовать. виа д’арженто! - добрый вечер...     9   \n",
       "2           2  27221  советовать вы увольнять ваш метродотель елена,...     9   \n",
       "3           3  29097  отличный средне вековый интеръер. приятный обс...     8   \n",
       "4           4  23065  ужинать в ресторан баден-баден 6 март . импоза...    10   \n",
       "\n",
       "   interior  service  \n",
       "0         8        8  \n",
       "1         7       10  \n",
       "2         9        1  \n",
       "3        10        9  \n",
       "4         8        8  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lemm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '.'.join(corpus_lemm['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_list = [x.strip() for x in text.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# notice, that our dataset has window=2\n",
    "\n",
    "w2v_cbow = Word2VecWrapper(window=2, negative=5, size=300, iter=300, is_cbow=True, random_state=SEED)\n",
    "w2v_cbow.fit(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(w2v_cbow, open(\"w2v_cbow\", \"wb\"))\n",
    "w2v_cbow = pickle.load(open(\"w2v_cbow\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cbow.has('еда')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_sentences_list = [x.split(' ') for x in sentences_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(words_sentences_list)):\n",
    "    for j in range(len(words_sentences_list[i])):\n",
    "        words_sentences_list[i][j] = preprocessing(words_sentences_list[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_sentences_list = [word for word in words_sentences_list if word != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec(words_sentences_list, size=500, window=10, min_count=2, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x149195b38>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(\"w2v_model\", \"wb\"))\n",
    "#model = pickle.load(open(\"w2v_model\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sent_words = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## пересечение со словарем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment_list-3.txt','r',encoding='utf-8') as file4:\n",
    "    new_sent_words = {word.split('\\t')[0] for word in file4.readlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего найдено  197  слов\n",
      "Пересечение со словарем оценочных выражений:  57 слов,  30.0 % списка\n"
     ]
    }
   ],
   "source": [
    "print('Всего найдено ', len(new_sent_words), ' слов')\n",
    "print('Пересечение со словарем оценочных выражений: ', len(sentiment_dict & new_sent_words), 'слов, ',\n",
    "      round(len(sentiment_dict & new_sent_words)/len(new_sent_words), 1)*100, '% списка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "неплохой\n",
      "признательность\n",
      "великолепный\n",
      "внимательный\n",
      "двойственный\n",
      "вкусный\n",
      "благодарность\n",
      "классный\n",
      "ужасный\n",
      "замечательный\n",
      "отрицательный\n",
      "улыбаться\n",
      "душевный\n",
      "обходительный\n",
      "противоречивый\n",
      "приветливый\n",
      "стильный\n",
      "прекрасный\n",
      "потрясающий\n",
      "негативный\n",
      "оригинальный\n",
      "осадок\n",
      "умеренный\n",
      "отменный\n",
      "испортиться\n",
      "обширный\n",
      "предупредительный\n",
      "приличный\n",
      "восторженный\n",
      "дружелюбный\n",
      "отвратительный\n",
      "грубить\n",
      "шикарный\n",
      "отличный\n",
      "качественный\n",
      "потрясать\n",
      "доброжелательный\n",
      "достойный\n",
      "разумный\n",
      "благодарный\n",
      "восхитительный\n",
      "правильный\n",
      "вежливый\n",
      "демократичный\n",
      "улыбчивый\n",
      "положительный\n",
      "гуманный\n",
      "симпатичный\n",
      "удивительный\n",
      "превосходный\n",
      "спокойный\n",
      "тихий\n",
      "изумительный\n",
      "позитивный\n",
      "живой\n",
      "чудесный\n",
      "посредственный\n"
     ]
    }
   ],
   "source": [
    "for word in sentiment_dict & new_sent_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "разнообразие\n",
      "топтаться\n",
      "милый \n",
      "организованный\n",
      "спасибо \n",
      "фоновый\n",
      "мальчик\n",
      "выступать\n",
      "неплохой \n",
      "умница \n",
      "отвечать \n",
      "внимательный \n",
      "кавказский\n",
      "окраина\n",
      "итальянский\n",
      "разнообразный \n",
      "средний \n",
      "громко\n",
      "девушка\n",
      "классный \n",
      "бейджик\n",
      "сервис \n",
      "приемлимый\n",
      "официанка\n",
      "супер \n",
      "японский\n",
      "обычный\n",
      "бегать\n",
      "играть\n",
      "отвратительный \n",
      "Вю\n",
      "бармен \n",
      "бедненький \n",
      "ненавязчивый \n",
      "отличный \n",
      "нормальный \n",
      "кофейня\n",
      "воспоминание\n",
      "благодарность \n",
      "выключать\n",
      "великолепный \n",
      "улыбаться \n",
      "гауд\n",
      "четко \n",
      "врубать\n",
      "клиент\n",
      "парень\n",
      "музыка \n",
      "менеджер\n",
      "вкусно \n",
      " quot оставлять\n",
      "обслуживание \n",
      "подстреливать\n",
      "китайский\n",
      "администратор \n",
      "мексикан \n",
      "вкусный \n",
      "ребята\n",
      "официантка\n",
      "мероприятие \n",
      "вежливый \n",
      "разнообразно \n",
      "молодец \n",
      "эмоция\n",
      "коллектив\n",
      "сервис\n",
      "грузинский\n",
      "бейдж\n",
      "дешевый \n",
      "впечатление \n",
      "сытно \n",
      "хостесса\n",
      "девушка \n",
      "девочка\n",
      "приятный \n",
      "официант \n",
      "изменяться\n",
      "вежливо\n",
      "свадебный\n",
      "разговаривать\n",
      "обалденный\n",
      "купчино\n",
      "петь\n",
      "бармен\n",
      "соня \n",
      "официант\n",
      "айс\n",
      "потрясать \n",
      "оформлять\n",
      "громкий\n",
      "умничек \n",
      "ачма\n",
      "обстоятельно \n",
      "вопрос \n",
      "орать \n",
      "отменный \n",
      "персонал \n",
      "объяснять\n",
      "уединенный\n",
      "пепперони \n",
      "суша бар\n",
      "сытно\n",
      "стараться\n",
      "доброжелательно \n",
      "плохой \n",
      "узбекский\n",
      "благодарить\n",
      "манта \n",
      "вопрос\n",
      "задавать\n",
      "хата \n",
      "персонал\n",
      "вкусно\n",
      "эмоция \n",
      "тетка quot \n",
      "официантка \n",
      "изумительный \n",
      "мильфей\n",
      "подготовка\n",
      "прекрасный \n",
      "поблагодарить\n",
      "обслуживать \n",
      "администратор\n",
      "достойный \n",
      "хороший \n",
      "витражный\n",
      "положительный \n",
      "хромать\n",
      "управляющий\n",
      "приличный \n",
      "басс\n",
      "периодически\n",
      "замечательный \n",
      "выражать\n",
      "уютный \n",
      " 320р \n",
      "пытаться\n",
      "красивый \n",
      "атмосферный\n",
      "марципановый\n"
     ]
    }
   ],
   "source": [
    "for word in (new_sent_words - sentiment_dict):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
